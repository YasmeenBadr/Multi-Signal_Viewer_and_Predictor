<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Voice Processing Suite</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@100..900&display=swap" rel="stylesheet">
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        body {
            /* Applied Inter font family */
            font-family: 'Inter', sans-serif; 
            /* Subtle, dark radial gradient background for a "digital" feel */
            background: radial-gradient(circle at top, #121212 0%, #0a0a0a 100%);
            color: white;
            overflow-x: hidden;
        }
        .main-color {
            color: #00eaff; /* Neon Cyan */
        }
        .glass-card {
            /* Defines the "Glassmorphism" effect */
            backdrop-filter: blur(12px);
            background: rgba(31, 31, 31, 0.7);
            /* Subtle neon border */
            border: 1px solid rgba(0, 234, 255, 0.2);
            box-shadow: 0 0 20px rgba(0, 234, 255, 0.05);
            transition: all 0.4s ease;
        }
        .glass-card:hover {
            border-color: rgba(0, 234, 255, 0.5);
            box-shadow: 0 0 25px rgba(0, 234, 255, 0.15);
            transform: translateY(-2px);
        }
        .drag-area {
            /* Neon dashed border for the active/upload area */
            border: 2px dashed #00eaff;
            background-color: rgba(15, 15, 15, 0.8);
            transition: 0.4s ease;
            backdrop-filter: blur(6px);
        }
        .drag-area.drag-over {
            /* Active state for drag-over */
            background-color: rgba(0, 234, 255, 0.1);
            border-color: #00eaff;
            box-shadow: 0 0 12px rgba(0, 234, 255, 0.3);
        }
        /* Custom styling for the range slider thumb for neon look */
        input[type=range]::-webkit-slider-thumb {
            -webkit-appearance: none;
            width: 18px;
            height: 18px;
            border-radius: 50%;
            background: #00eaff;
            cursor: pointer;
            box-shadow: 0 0 10px #00eaff;
        }
        .btn-control {
            /* NEW: Uniform, minimalist button styling */
            background: rgba(45, 45, 45, 0.8); /* Darker glass background */
            border: 1px solid rgba(100, 100, 100, 0.4);
            color: #fff;
            transition: all 0.3s ease;
            position: relative;
        }
        /* NEW: Neon glow border on hover is kept, but with a universal background */
        .btn-control::after {
            content: "";
            position: absolute;
            inset: 0;
            border-radius: 9999px;
            border: 1px solid rgba(0, 234, 255, 0.2);
            transition: all 0.3s ease;
        }
        .btn-control:hover {
            background: rgba(60, 60, 60, 0.9);
        }
        .btn-control:hover::after {
            border-color: rgba(0, 234, 255, 0.6);
            box-shadow: 0 0 10px rgba(0, 234, 255, 0.3);
        }
        /* NEW: Removed text-shadow for main title */
        .neon-title {
            /* text-shadow removed */
        }
        
        /* NEW: Gender-specific styles for the classification results container */
        .results-male {
            border-color: #3b82f6; /* Blue-500 */
            box-shadow: 0 0 25px rgba(59, 130, 246, 0.25);
        }
        .results-female {
            border-color: #ec4899; /* Pink-500 */
            box-shadow: 0 0 25px rgba(236, 72, 153, 0.25);
        }

        /* Creative accent bar */
        .gender-accent-bar {
            height: 8px;
            width: 100%;
            border-radius: 9999px;
            margin-bottom: 20px;
            transition: background 0.5s ease;
        }
        .gender-accent-bar-male {
            background: linear-gradient(90deg, #3b82f6 0%, #00eaff 100%);
        }
        .gender-accent-bar-female {
            background: linear-gradient(90deg, #ec4899 0%, #f472b6 100%);
        }

        /* Recording indicator animation */
        @keyframes pulse-red {
            0%, 100% { opacity: 1; }
            50% { opacity: 0.5; }
        }
        .recording-indicator {
            animation: pulse-red 1.5s ease-in-out infinite;
        }
        
        .record-btn-active {
            background: rgba(220, 38, 38, 0.9) !important;
            border-color: rgba(239, 68, 68, 0.6) !important;
        }
    </style>
</head>

<body class="min-h-screen flex flex-col items-center justify-center p-6">
    <div class="max-w-5xl w-full text-center space-y-8 animate-fade-in">
        
        <header class="mb-12">
            <h1 class="text-5xl md:text-6xl font-extrabold main-color tracking-wide">
                Voice Processing Suite
            </h1>
            <p class="text-gray-300 text-lg md:text-xl mt-4">
                Experiment with audio fidelity and discover how **sampling rates** affect sound clarity.
            </p>
        </header>

        <div class="glass-card rounded-3xl p-8 md:p-12 shadow-2xl mx-auto w-full">
            <h2 class="text-3xl font-semibold mb-8 text-white">Sampling Rate Demonstrator & Analysis</h2>

            <section class="mb-10">
                <!-- Voice Recording Section -->
                <div class="mb-8 p-6 rounded-2xl bg-gray-800 bg-opacity-50 border border-gray-700">
                    <h3 class="text-xl font-semibold mb-4 text-white flex items-center justify-center gap-2">
                        <svg class="w-6 h-6 text-red-400" fill="currentColor" viewBox="0 0 20 20">
                            <path fill-rule="evenodd" d="M7 4a3 3 0 016 0v4a3 3 0 11-6 0V4zm4 10.93A7.001 7.001 0 0017 8a1 1 0 10-2 0A5 5 0 015 8a1 1 0 00-2 0 7.001 7.001 0 006 6.93V17H6a1 1 0 100 2h8a1 1 0 100-2h-3v-2.07z" clip-rule="evenodd"></path>
                        </svg>
                        Record Your Voice
                    </h3>
                    <div class="flex flex-col items-center gap-4">
                        <div class="flex gap-3">
                            <button id="recordBtn" class="btn-control text-white font-bold py-3 px-8 rounded-full flex items-center gap-2">
                                <svg class="w-5 h-5" fill="currentColor" viewBox="0 0 20 20">
                                    <path fill-rule="evenodd" d="M10 18a8 8 0 100-16 8 8 0 000 16zM9.555 7.168A1 1 0 008 8v4a1 1 0 001.555.832l3-2a1 1 0 000-1.664l-3-2z" clip-rule="evenodd"></path>
                                </svg>
                                Start Recording
                            </button>
                            <button id="stopRecordBtn" class="btn-control text-white font-bold py-3 px-8 rounded-full flex items-center gap-2 disabled:opacity-30" disabled>
                                <svg class="w-5 h-5" fill="currentColor" viewBox="0 0 20 20">
                                    <path fill-rule="evenodd" d="M10 18a8 8 0 100-16 8 8 0 000 16zM8 7a1 1 0 00-1 1v4a1 1 0 001 1h4a1 1 0 001-1V8a1 1 0 00-1-1H8z" clip-rule="evenodd"></path>
                                </svg>
                                Stop Recording
                            </button>
                        </div>
                        <div id="recordingStatus" class="text-sm text-gray-400 h-6"></div>
                        <div id="recordingTimer" class="hidden text-2xl font-mono main-color">00:00</div>
                    </div>
                </div>

                <!-- File Upload Section -->
                <div class="text-center mb-4">
                    <p class="text-gray-400 text-sm">OR</p>
                </div>
                <div id="dragDropArea" class="drag-area p-10 rounded-2xl text-center cursor-pointer hover:shadow-lg" onclick="document.getElementById('fileInput').click()">
                    <p id="filePrompt" class="text-gray-300 font-medium">
                        **Drag & Drop** an MP3/WAV audio file here, or **click to upload**.
                    </p>
                    <p id="fileNameDisplay" class="text-green-400 mt-3 hidden text-sm font-mono"></p>
                    <input type="file" id="fileInput" accept="audio/*" class="hidden">
                </div>
            </section>

            <section id="controls" class="space-y-10 opacity-50 pointer-events-none transition-all duration-700">
                
                <div class="flex flex-col md:flex-row items-center justify-between gap-6 p-4 rounded-xl bg-gray-800 bg-opacity-50">
                    <label for="samplingFrequency" class="text-gray-300 font-semibold w-full md:w-1/3 text-left">Target Sample Rate:</label>
                    <div class="w-full md:w-2/3">
                        <input type="range" id="samplingFrequency" min="4000" max="44100" value="44100" step="100" class="w-full h-2 bg-gray-700 rounded-lg appearance-none cursor-pointer">
                        <p class="text-sm text-gray-400 mt-2 text-right"><span id="currentFrequency" class="main-color font-bold">44100</span> Hz (Original Rate)</p>
                    </div>
                </div>

                <!-- Anti-Aliasing Filter Toggle -->
                <div class="flex items-center justify-between p-4 rounded-xl bg-gradient-to-r from-purple-900 to-indigo-900 bg-opacity-50 border border-purple-500">
                    <div class="flex items-center gap-3">
                        <svg class="w-6 h-6 text-purple-400" fill="currentColor" viewBox="0 0 20 20"><path fill-rule="evenodd" d="M11.3 1.046A1 1 0 0112 2v5h4a1 1 0 01.82 1.573l-7 10A1 1 0 018 18v-5H4a1 1 0 01-.82-1.573l7-10a1 1 0 011.12-.38z" clip-rule="evenodd"></path></svg>
                        <div>
                            <label for="antiAliasingToggle" class="text-white font-semibold cursor-pointer">Anti-Aliasing Filter</label>
                            <p class="text-xs text-gray-300">Apply low-pass filter before downsampling</p>
                        </div>
                    </div>
                    <label class="relative inline-flex items-center cursor-pointer">
                        <input type="checkbox" id="antiAliasingToggle" class="sr-only peer" checked>
                        <div class="w-14 h-7 bg-gray-700 peer-focus:outline-none peer-focus:ring-4 peer-focus:ring-purple-800 rounded-full peer peer-checked:after:translate-x-full peer-checked:after:border-white after:content-[''] after:absolute after:top-0.5 after:left-[4px] after:bg-white after:border-gray-300 after:border after:rounded-full after:h-6 after:w-6 after:transition-all peer-checked:bg-purple-600"></div>
                    </label>
                </div>

                <div class="flex justify-center flex-wrap gap-4 pt-4 border-t border-gray-700">
                    <button id="playOriginalBtn" class="btn-control text-white font-bold py-3 px-8 rounded-full flex items-center disabled:opacity-50" disabled>
                        Start Original
                    </button>
                    <button id="pauseBtn" class="btn-control text-white font-bold py-3 px-8 rounded-full flex items-center disabled:opacity-50" disabled>
                        Pause / Stop
                    </button>
                    <button id="playSampledBtn" class="btn-control text-white font-bold py-3 px-8 rounded-full flex items-center disabled:opacity-50" disabled>
                        Start Resampled
                    </button>
                    <button id="playReconstructedBtn" class="btn-control text-white font-bold py-3 px-8 rounded-full flex items-center disabled:opacity-50" disabled>
                        <svg class="w-5 h-5 mr-2" fill="currentColor" viewBox="0 0 20 20"><path fill-rule="evenodd" d="M11.3 1.046A1 1 0 0112 2v5h4a1 1 0 01.82 1.573l-7 10A1 1 0 018 18v-5H4a1 1 0 01-.82-1.573l7-10a1 1 0 011.12-.38z" clip-rule="evenodd"></path></svg>
                        Play Reconstructed
                    </button>
                </div>

                <div class="grid grid-cols-2 md:grid-cols-4 gap-4 pt-6 border-t border-gray-700 mt-8">
                    <button id="classifyOriginalBtn" class="btn-control text-white font-bold py-3 px-6 rounded-full flex items-center justify-center disabled:opacity-50" disabled>
                        <svg class="w-5 h-5 mr-2" fill="currentColor" viewBox="0 0 20 20"><path d="M9 2a1 1 0 000 2h2a1 1 0 100-2H9z"></path><path fill-rule="evenodd" d="M4 5a2 2 0 012-2 3 3 0 003 3h2a3 3 0 003-3 2 2 0 012 2v11a2 2 0 01-2 2H6a2 2 0 01-2-2V5zm3 4a1 1 0 000 2h.01a1 1 0 100-2H7zm3 0a1 1 0 000 2h3a1 1 0 100-2h-3zm-3 4a1 1 0 100 2h.01a1 1 0 100-2H7zm3 0a1 1 0 100 2h3a1 1 0 100-2h-3z" clip-rule="evenodd"></path></svg>
                        Original
                    </button>
                    
                    <button id="classifyResampledBtn" class="btn-control text-white font-bold py-3 px-6 rounded-full flex items-center justify-center disabled:opacity-50" disabled>
                        <svg class="w-5 h-5 mr-2" fill="currentColor" viewBox="0 0 20 20"><path fill-rule="evenodd" d="M6 2a2 2 0 00-2 2v12a2 2 0 002 2h8a2 2 0 002-2V7.414A2 2 0 0015.414 6L12 2.586A2 2 0 0010.586 2H6zm5 6a1 1 0 10-2 0v3.586l-1.293-1.293a1 1 0 10-1.414 1.414l3 3a1 1 0 001.414 0l3-3a1 1 0 00-1.414-1.414L11 11.586V8z" clip-rule="evenodd"></path></svg>
                        Resampled
                    </button>
                    
                    <button id="classifyReconstructedBtn" class="btn-control text-white font-bold py-3 px-6 rounded-full flex items-center justify-center disabled:opacity-50" disabled>
                        <svg class="w-5 h-5 mr-2" fill="currentColor" viewBox="0 0 20 20"><path fill-rule="evenodd" d="M11.3 1.046A1 1 0 0112 2v5h4a1 1 0 01.82 1.573l-7 10A1 1 0 018 18v-5H4a1 1 0 01-.82-1.573l7-10a1 1 0 011.12-.38z" clip-rule="evenodd"></path></svg>
                        Reconstructed
                    </button>
                    
                    <button id="resetBtn" class="btn-control text-white font-bold py-3 px-6 rounded-full flex items-center justify-center disabled:opacity-40">
                        <svg class="w-5 h-5 mr-2" fill="none" stroke="currentColor" stroke-width="2" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" d="M4 4v5h.582m15.356 2A8.001 8.001 0 0014 4H5m7 12l-4-4m0 0l-4 4m4-4V4"></path></svg>
                        Reset
                    </button>
                </div>

                <!-- Original Audio Classification Results -->
                <div id="originalResults" class="hidden mt-8 p-6 bg-gray-800 bg-opacity-70 rounded-2xl border-2 transition-all duration-500">
                    <div id="originalAccentBar" class="gender-accent-bar"></div>
                    <h3 class="text-xl font-bold mb-4 text-white flex items-center gap-2">
                        <svg class="w-5 h-5 text-green-400" fill="currentColor" viewBox="0 0 20 20"><path d="M9 2a1 1 0 000 2h2a1 1 0 100-2H9z"></path><path fill-rule="evenodd" d="M4 5a2 2 0 012-2 3 3 0 003 3h2a3 3 0 003-3 2 2 0 012 2v11a2 2 0 01-2 2H6a2 2 0 01-2-2V5zm3 4a1 1 0 000 2h.01a1 1 0 100-2H7zm3 0a1 1 0 000 2h3a1 1 0 100-2h-3zm-3 4a1 1 0 100 2h.01a1 1 0 100-2H7zm3 0a1 1 0 100 2h3a1 1 0 100-2h-3z" clip-rule="evenodd"></path></svg>
                        Original Audio Classification
                    </h3>
                    <div class="grid grid-cols-1 md:grid-cols-3 gap-4 text-center">
                        <div>
                            <p class="text-gray-400 text-sm mb-1">Gender</p>
                            <p id="originalGender" class="text-2xl font-bold">N/A</p>
                        </div>
                        <div>
                            <p class="text-gray-400 text-sm mb-1">Confidence</p>
                            <p id="originalConfidence" class="text-2xl font-bold text-green-400">N/A</p>
                        </div>
                        <div>
                            <p class="text-gray-400 text-sm mb-1">Avg Pitch (F0)</p>
                            <p id="originalPitch" class="text-2xl font-bold text-cyan-400">N/A</p>
                        </div>
                    </div>
                    <div class="mt-3 text-sm text-gray-400">
                        Sample Rate: <span id="originalSampleRate" class="text-cyan-400 font-mono">N/A</span> Hz
                    </div>
                </div>

                <!-- Resampled Audio Classification Results -->
                <div id="resampledResults" class="hidden mt-6 p-6 bg-gray-800 bg-opacity-70 rounded-2xl border-2 transition-all duration-500">
                    <div id="resampledAccentBar" class="gender-accent-bar"></div>
                    <h3 class="text-xl font-bold mb-4 text-white flex items-center gap-2">
                        <svg class="w-5 h-5 text-yellow-400" fill="currentColor" viewBox="0 0 20 20"><path fill-rule="evenodd" d="M6 2a2 2 0 00-2 2v12a2 2 0 002 2h8a2 2 0 002-2V7.414A2 2 0 0015.414 6L12 2.586A2 2 0 0010.586 2H6zm5 6a1 1 0 10-2 0v3.586l-1.293-1.293a1 1 0 10-1.414 1.414l3 3a1 1 0 001.414 0l3-3a1 1 0 00-1.414-1.414L11 11.586V8z" clip-rule="evenodd"></path></svg>
                        Resampled Audio Classification
                    </h3>
                    <div class="grid grid-cols-1 md:grid-cols-3 gap-4 text-center">
                        <div>
                            <p class="text-gray-400 text-sm mb-1">Gender</p>
                            <p id="resampledGender" class="text-2xl font-bold">N/A</p>
                        </div>
                        <div>
                            <p class="text-gray-400 text-sm mb-1">Confidence</p>
                            <p id="resampledConfidence" class="text-2xl font-bold text-green-400">N/A</p>
                        </div>
                        <div>
                            <p class="text-gray-400 text-sm mb-1">Avg Pitch (F0)</p>
                            <p id="resampledPitch" class="text-2xl font-bold text-cyan-400">N/A</p>
                        </div>
                    </div>
                    <div class="mt-3 text-sm text-gray-400">
                        Sample Rate: <span id="resampledSampleRate" class="text-yellow-400 font-mono">N/A</span> Hz
                    </div>
                </div>

                <!-- Reconstructed Audio Classification Results -->
                <div id="reconstructedResults" class="hidden mt-6 p-6 bg-gradient-to-r from-purple-900 to-indigo-900 bg-opacity-70 rounded-2xl border-2 border-purple-500 transition-all duration-500">
                    <div id="reconstructedAccentBar" class="gender-accent-bar"></div>
                    <h3 class="text-xl font-bold mb-4 text-white flex items-center gap-2">
                        <svg class="w-5 h-5 text-purple-400" fill="currentColor" viewBox="0 0 20 20"><path fill-rule="evenodd" d="M11.3 1.046A1 1 0 0112 2v5h4a1 1 0 01.82 1.573l-7 10A1 1 0 018 18v-5H4a1 1 0 01-.82-1.573l7-10a1 1 0 011.12-.38z" clip-rule="evenodd"></path></svg>
                        Reconstructed Audio Classification (Anti-Aliased)
                    </h3>
                    <div class="grid grid-cols-1 md:grid-cols-3 gap-4 text-center">
                        <div>
                            <p class="text-gray-400 text-sm mb-1">Gender</p>
                            <p id="reconstructedGender" class="text-2xl font-bold">N/A</p>
                        </div>
                        <div>
                            <p class="text-gray-400 text-sm mb-1">Confidence</p>
                            <p id="reconstructedConfidence" class="text-2xl font-bold text-green-400">N/A</p>
                        </div>
                        <div>
                            <p class="text-gray-400 text-sm mb-1">Avg Pitch (F0)</p>
                            <p id="reconstructedPitch" class="text-2xl font-bold text-cyan-400">N/A</p>
                        </div>
                    </div>
                    <div class="mt-3 text-sm text-gray-400">
                        Sample Rate: <span id="reconstructedSampleRate" class="text-purple-400 font-mono">N/A</span> Hz (Upsampled from <span id="reconstructedOriginalRate" class="text-purple-400 font-mono">N/A</span> Hz)
                    </div>
                </div>

                <!-- Aliasing Analysis -->
                <div id="aliasingAnalysis" class="hidden mt-6 p-6 bg-gradient-to-r from-purple-900 to-indigo-900 bg-opacity-50 rounded-2xl border-2 border-purple-500">
                    <h3 class="text-xl font-bold mb-4 text-white flex items-center gap-2">
                        <svg class="w-5 h-5 text-purple-400" fill="currentColor" viewBox="0 0 20 20"><path fill-rule="evenodd" d="M18 10a8 8 0 11-16 0 8 8 0 0116 0zm-7-4a1 1 0 11-2 0 1 1 0 012 0zM9 9a1 1 0 000 2v3a1 1 0 001 1h1a1 1 0 100-2v-3a1 1 0 00-1-1H9z" clip-rule="evenodd"></path></svg>
                        Aliasing & Under-sampling Analysis
                    </h3>
                    <div class="space-y-3 text-sm">
                        <div class="flex justify-between items-center p-3 bg-black bg-opacity-30 rounded-lg">
                            <span class="text-gray-300">Classification Match:</span>
                            <span id="classificationMatch" class="font-bold text-lg">N/A</span>
                        </div>
                        <div class="flex justify-between items-center p-3 bg-black bg-opacity-30 rounded-lg">
                            <span class="text-gray-300">Confidence Change:</span>
                            <span id="confidenceChange" class="font-bold text-lg">N/A</span>
                        </div>
                        <div class="flex justify-between items-center p-3 bg-black bg-opacity-30 rounded-lg">
                            <span class="text-gray-300">Pitch Change:</span>
                            <span id="pitchChange" class="font-bold text-lg">N/A</span>
                        </div>
                        <div class="flex justify-between items-center p-3 bg-black bg-opacity-30 rounded-lg">
                            <span class="text-gray-300">Nyquist Frequency:</span>
                            <span id="nyquistFreq" class="font-bold text-lg text-cyan-400">N/A</span>
                        </div>
                        <div class="p-3 bg-black bg-opacity-30 rounded-lg">
                            <p class="text-gray-300 mb-1">Aliasing Effect:</p>
                            <p id="aliasingEffect" class="font-semibold text-white">N/A</p>
                        </div>
                    </div>
                </div>

                <p id="messageBox" class="text-sm text-red-400 mt-4 h-5"></p>
            </section>
        </div>

        <a href="/" class="inline-block mt-8 text-lg bg-gradient-to-r from-cyan-600 to-blue-600 hover:from-cyan-500 hover:to-blue-500 text-white font-bold py-3 px-8 rounded-full transition-transform hover:scale-105 shadow-lg">
            ← Back to Dashboard
        </a>
    </div>

    <script>
        let audioContext;
        let originalBuffer = null;
        let resampledBuffer = null;
        let resampledLowRateBuffer = null;
        let reconstructedBuffer = null;
        let originalRate = 44100;
        let isPlaying = false;
        let currentSourceNode = null;
        let uploadedFile = null; // Store the uploaded file for classification

        // Recording variables
        let mediaRecorder = null;
        let audioChunks = [];
        let recordingStream = null;
        let recordingStartTime = null;
        let recordingTimerInterval = null;

        const fileInput = document.getElementById('fileInput');
        const dragDropArea = document.getElementById('dragDropArea');
        const filePrompt = document.getElementById('filePrompt');
        const fileNameDisplay = document.getElementById('fileNameDisplay');
        const controls = document.getElementById('controls');
        const frequencySlider = document.getElementById('samplingFrequency');
        const currentFrequencyDisplay = document.getElementById('currentFrequency');
        const playOriginalBtn = document.getElementById('playOriginalBtn');
        const playSampledBtn = document.getElementById('playSampledBtn');
        const playReconstructedBtn = document.getElementById('playReconstructedBtn');
        const pauseBtn = document.getElementById('pauseBtn');
        const resetBtn = document.getElementById('resetBtn');
        const classifyOriginalBtn = document.getElementById('classifyOriginalBtn');
        const classifyResampledBtn = document.getElementById('classifyResampledBtn');
        const classifyReconstructedBtn = document.getElementById('classifyReconstructedBtn');
        const antiAliasingToggle = document.getElementById('antiAliasingToggle');
        const messageBox = document.getElementById('messageBox');
        
        // Original results elements
        const originalResults = document.getElementById('originalResults');
        const originalGender = document.getElementById('originalGender');
        const originalConfidence = document.getElementById('originalConfidence');
        const originalPitch = document.getElementById('originalPitch');
        const originalSampleRate = document.getElementById('originalSampleRate');
        const originalAccentBar = document.getElementById('originalAccentBar');
        
        // Resampled results elements
        const resampledResults = document.getElementById('resampledResults');
        const resampledGender = document.getElementById('resampledGender');
        const resampledConfidence = document.getElementById('resampledConfidence');
        const resampledPitch = document.getElementById('resampledPitch');
        const resampledSampleRate = document.getElementById('resampledSampleRate');
        const resampledAccentBar = document.getElementById('resampledAccentBar');
        
        // Reconstructed results elements
        const reconstructedResults = document.getElementById('reconstructedResults');
        const reconstructedGender = document.getElementById('reconstructedGender');
        const reconstructedConfidence = document.getElementById('reconstructedConfidence');
        const reconstructedPitch = document.getElementById('reconstructedPitch');
        const reconstructedSampleRate = document.getElementById('reconstructedSampleRate');
        const reconstructedOriginalRate = document.getElementById('reconstructedOriginalRate');
        const reconstructedAccentBar = document.getElementById('reconstructedAccentBar');
        
        // Aliasing analysis elements
        const aliasingAnalysis = document.getElementById('aliasingAnalysis');
        const classificationMatch = document.getElementById('classificationMatch');
        const confidenceChange = document.getElementById('confidenceChange');
        const pitchChange = document.getElementById('pitchChange');
        const nyquistFreq = document.getElementById('nyquistFreq');
        const aliasingEffect = document.getElementById('aliasingEffect');
        
        const recordBtn = document.getElementById('recordBtn');
        const stopRecordBtn = document.getElementById('stopRecordBtn');
        const recordingStatus = document.getElementById('recordingStatus');
        const recordingTimer = document.getElementById('recordingTimer');
        
        // Store classification results for comparison
        let originalClassification = null;
        let resampledClassification = null;
        let reconstructedClassification = null;

        // Initialize AudioContext on first interaction
        function initAudioContext() {
            if (!audioContext) {
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                // Update slider max value to the native rate upon initialization
                frequencySlider.max = audioContext.sampleRate;
                frequencySlider.value = audioContext.sampleRate;
                currentFrequencyDisplay.textContent = audioContext.sampleRate;
                messageBox.textContent = `Native Sample Rate: ${audioContext.sampleRate} Hz`;
                originalRate = audioContext.sampleRate;
            }
        }
        
        // --- 1. Audio Loading and Decoding ---
/*Audio Input and Decoding: When a user uploads or records audio, 
the decodeAudio function is called. It uses the browser's built-in Web 
Audio API to convert the file into raw audio data that can be processed.*/
        function decodeAudio(file) {
            uploadedFile = file; // Store for classification later
            initAudioContext();
            const reader = new FileReader();

            reader.onload = function(event) {
                const arrayBuffer = event.target.result;
                // Decode the raw file data into a usable audio buffer
                audioContext.decodeAudioData(arrayBuffer, 
                    function(buffer) {
                        originalBuffer = buffer;
                        originalRate = buffer.sampleRate;
                        // Reset slider max/value based on the loaded file's rate
                        frequencySlider.max = originalRate;
                        frequencySlider.value = originalRate;
                        currentFrequencyDisplay.textContent = originalRate;
                        
                        resampleAudio(originalRate);
                        updateUI(file.name);
                    },
                    function(e) {
                        messageBox.textContent = "Error decoding audio data. Is the file corrupted?";
                        console.error("Audio decoding error:", e);
                    }
                );
            };

            reader.onerror = function() {
                messageBox.textContent = "Error reading file.";
            };

            reader.readAsArrayBuffer(file);
        }

        function updateUI(fileName) {
            controls.classList.remove('opacity-50', 'pointer-events-none');
            playOriginalBtn.disabled = false;
            playSampledBtn.disabled = false;
            playReconstructedBtn.disabled = false;
            resetBtn.disabled = false;
            classifyOriginalBtn.disabled = false;
            classifyResampledBtn.disabled = false;
            classifyReconstructedBtn.disabled = false;
            fileNameDisplay.textContent = `Loaded: ${fileName}`;
            fileNameDisplay.classList.remove('hidden');
            filePrompt.classList.add('hidden');
            messageBox.textContent = `File ready. Original rate: ${originalRate} Hz`;
        }

        // --- 2. Resampling Logic with Anti-Aliasing Filter ---
        
       // Defines the main function, which takes the desired new sample rate as input.
function resampleAudio(targetRate) {
    // If no audio has been loaded, stop the function immediately.
    if (!originalBuffer) return;

    // Checks if the "Anti-Aliasing" switch is turned on in the user interface.
    const useAntiAliasing = antiAliasingToggle.checked;
    // Calculates the ratio for scaling down. e.g., A ratio of 4 means we keep 1 out of every 4 samples.
    const ratio = originalRate / targetRate;
    
    // Calculates the duration of the audio clip in seconds.
    const duration = originalBuffer.length / originalRate;
    // Calculates the new no of samples of the downsampled audio clip in the same duration.
    const downsampledLength = Math.floor(duration * targetRate);

    // Creates a temporary array to hold the low-quality audio data for all channels.
    const tempDownsampled = [];
    
    // Starts a loop to process each audio channel separately.
    for (let channel = 0; channel < originalBuffer.numberOfChannels; channel++) {
        // Gets the raw audio data from the original, high-quality buffer for the current channel.
        let originalData = originalBuffer.getChannelData(channel);
        
        // If the user enabled anti-aliasing...
        if (useAntiAliasing) {
            // ...replace the original data with the smoothed version from our filter function.
            originalData = applyLowPassFilter(originalData, originalRate, targetRate / 2);
        }
        
        // Creates a new, empty array to store the final downsampled (low-quality) audio.
        const channelDownsampled = new Float32Array(downsampledLength);
        // This loop performs the actual downsampling.
        for (let i = 0; i < downsampledLength; i++) {
            // Calculates which sample to pick from the original audio by scaling up the index `i`.
            const originalIndex = Math.floor(i * ratio);
            // Copies that single sample, discarding all others in between.
            channelDownsampled[i] = originalData[Math.min(originalIndex, originalData.length - 1)];
        }
        // Adds the newly created low-quality channel data to our temporary array.
        tempDownsampled.push(channelDownsampled);
    }
    
    // --- At this point, `tempDownsampled` holds the low-quality audio data ---

    // Creates a new audio buffer specifically for the classification engine. This buffer has the *true* low sample rate.
    resampledLowRateBuffer = audioContext.createBuffer(
        originalBuffer.numberOfChannels,
        downsampledLength,
        targetRate // Note: the sample rate is the new, low targetRate
    );
    // This loop copies the low-quality data into the new buffer.
    for (let channel = 0; channel < originalBuffer.numberOfChannels; channel++) {
        const dst = resampledLowRateBuffer.getChannelData(channel);
        dst.set(tempDownsampled[channel]);
    }

    // Now, create a SEPARATE buffer for playback. This one is upsampled back to the original length and rate.
    resampledBuffer = audioContext.createBuffer(
        originalBuffer.numberOfChannels,
        originalBuffer.length, // Back to the original full length
        originalBuffer.sampleRate // Back to the original high sample rate
    );

    // Loop through each channel to perform the upsampling for playback.
    for (let channel = 0; channel < originalBuffer.numberOfChannels; channel++) {
        // Gets the low-quality data for the current channel.
        const downsampledData = tempDownsampled[channel];
        // Gets a reference to the output array where the upsampled data will be written.
        const resampledData = resampledBuffer.getChannelData(channel);
        
        // This loop performs a simple "zero-order hold" upsampling, which sounds harsh and robotic.
        // It's used to demonstrate what happens without proper interpolation.
        for (let i = 0; i < resampledData.length; i++) {
            // Calculates which sample from the small, low-quality array to use.
            const downsampledIndex = Math.floor(i / ratio);
            // Copies that sample. This same sample will be copied multiple times in a row, creating a "stair-step" effect.
            resampledData[i] = downsampledData[Math.min(downsampledIndex, downsampledData.length - 1)];
        }
    }

    // Finally, if anti-aliasing was used...
    if (useAntiAliasing) {
        // ...call the high-quality reconstruction function to create a third, clean-sounding version of the audio.
        reconstructSignal(targetRate, tempDownsampled);
    }
}

        // Low-pass filter using simple moving average (anti-aliasing filter)
        function applyLowPassFilter(data, sampleRate, cutoffFreq) {
            const filtered = new Float32Array(data.length);
            const filterOrder = Math.floor(sampleRate / cutoffFreq);
            
            for (let i = 0; i < data.length; i++) {
                let sum = 0;
                let count = 0;
                
                for (let j = Math.max(0, i - filterOrder); j <= Math.min(data.length - 1, i + filterOrder); j++) {
                    sum += data[j];
                    count++;
                }
                
                filtered[i] = sum / count;
            }
            
            return filtered;
        }

        // Reconstruct signal using linear interpolation (upsampling)
       // Defines the function 'reconstructSignal'.
function reconstructSignal(downsampledRate, tempDownsampled) {
    // A "guard clause" that stops the function if there's no data to process.
    if (!tempDownsampled) return;

    // Recalculates the ratio needed to scale the short clip back up to the long one.
    const ratio = originalRate / downsampledRate;
    // Gets the target length, which is the length of the original high-quality audio.
    const reconstructedLength = originalBuffer.length;

    // Creates a new, empty, full-length audio buffer to store the final high-quality result.
    reconstructedBuffer = audioContext.createBuffer(
        originalBuffer.numberOfChannels,
        reconstructedLength,
        originalBuffer.sampleRate
    );

    // Starts a loop to process each audio channel (e.g., left and right for stereo).
    for (let channel = 0; channel < originalBuffer.numberOfChannels; channel++) {
        // Gets the low-quality audio data for the current channel.
        const downsampledData = tempDownsampled[channel];
        // Gets a reference to the output array where the new, high-quality data will be written.
        const reconstructedData = reconstructedBuffer.getChannelData(channel);

        // Starts a loop to generate every single sample for the final, full-length audio.
        for (let i = 0; i < reconstructedLength; i++) {
            // Calculates the "ideal" position in the small, low-quality array. This will be a decimal number (e.g., 5.7).
            const downsampledIndex = i / ratio;
            // Finds the index of the known sample to the LEFT of our ideal position (e.g., 5).
            const lowerIndex = Math.floor(downsampledIndex);
            // Finds the index of the known sample to the RIGHT of our ideal position (e.g., 6).
            const upperIndex = Math.min(lowerIndex + 1, downsampledData.length - 1);
            // Calculates how far we are between the left and right points (e.g., 0.7 for 70% of the way).
            const fraction = downsampledIndex - lowerIndex;

            // This is the linear interpolation formula. It creates a new sample value by taking a weighted average
            // of the left and right samples, effectively drawing a straight line between them.
            reconstructedData[i] = downsampledData[lowerIndex] * (1 - fraction) + 
                                  downsampledData[upperIndex] * fraction;
        }
    }
}
        
        // --- 3. Gender Classification Functions ---
        
        async function classifyOriginal() {
            if (!uploadedFile) {
                messageBox.textContent = "No file uploaded!";
                return;
            }

            classifyOriginalBtn.disabled = true;
            messageBox.textContent = "Classifying original audio... Please wait.";

            const formData = new FormData();
            formData.append('audio', uploadedFile);

            try {
                const response = await fetch('/voice/classify', {
                    method: 'POST',
                    body: formData
                });

                const data = await response.json();
                
                if (!response.ok) {
                    throw new Error(data.error || 'Classification failed');
                }
                
                // Store and display results
                originalClassification = {
                    gender: data.gender,
                    confidence: data.confidence,
                    pitch: data.pitch,
                    sampleRate: originalRate
                };
                
                displayOriginalResults(originalClassification);
                messageBox.textContent = "Original audio classified successfully!";
                
                // Show analysis if both classifications are done
                if (resampledClassification) {
                    displayAliasingAnalysis();
                }
            } catch (error) {
                messageBox.textContent = "Error: " + error.message;
                console.error('Classification error:', error);
            } finally {
                classifyOriginalBtn.disabled = false;
            }
        }

        async function classifyResampled() {
            if (!resampledBuffer) {
                messageBox.textContent = "No resampled audio available!";
                return;
            }
            if (!resampledLowRateBuffer) {
                messageBox.textContent = "Internal error: low-rate buffer missing. Move the sampling slider and try again.";
                return;
            }

            classifyResampledBtn.disabled = true;
            messageBox.textContent = "Classifying resampled audio... Please wait.";

            try {
                // Convert TRUE downsampled buffer (with low sample rate) to WAV file
                const wavBlob = audioBufferToWav(resampledLowRateBuffer);
                const wavFile = new File([wavBlob], 'resampled_audio.wav', { type: 'audio/wav' });
                
                const formData = new FormData();
                formData.append('audio', wavFile);
                // Include effective sample rate used before reconstruction
                const effectiveSr2 = parseInt(frequencySlider.value);
                formData.append('effective_sr', String(effectiveSr2));
                // Pass effective sample rate to backend for aliasing-aware behavior
                const effectiveSr = parseInt(frequencySlider.value);
                formData.append('effective_sr', String(effectiveSr));

                const response = await fetch('/voice/classify', {
                    method: 'POST',
                    body: formData
                });

                const data = await response.json();
                
                if (!response.ok) {
                    throw new Error(data.error || 'Classification failed');
                }
                
                // Store and display results
                const targetRate = parseInt(frequencySlider.value);
                resampledClassification = {
                    gender: data.gender,
                    confidence: data.confidence,
                    pitch: data.pitch,
                    sampleRate: targetRate
                };
                
                displayResampledResults(resampledClassification);
                messageBox.textContent = "Resampled audio classified successfully!";
                
                // Show analysis if both classifications are done
                if (originalClassification) {
                    displayAliasingAnalysis();
                }
            } catch (error) {
                messageBox.textContent = "Error: " + error.message;
                console.error('Classification error:', error);
            } finally {
                classifyResampledBtn.disabled = false;
            }
        }

        async function classifyReconstructed() {
            if (!reconstructedBuffer) {
                messageBox.textContent = "No reconstructed audio available! Enable anti-aliasing filter.";
                return;
            }

            classifyReconstructedBtn.disabled = true;
            messageBox.textContent = "Classifying reconstructed audio... Please wait.";

            try {
                // Convert reconstructed buffer to WAV file
                const wavBlob = audioBufferToWav(reconstructedBuffer);
                const wavFile = new File([wavBlob], 'reconstructed_audio.wav', { type: 'audio/wav' });
                
                const formData = new FormData();
                formData.append('audio', wavFile);

                const response = await fetch('/voice/classify', {
                    method: 'POST',
                    body: formData
                });

                const data = await response.json();
                
                if (!response.ok) {
                    throw new Error(data.error || 'Classification failed');
                }
                
                // Store and display results
                const targetRate = parseInt(frequencySlider.value);
                reconstructedClassification = {
                    gender: data.gender,
                    confidence: data.confidence,
                    pitch: data.pitch,
                    sampleRate: originalRate,
                    downsampledFrom: targetRate
                };
                
                displayReconstructedResults(reconstructedClassification);
                messageBox.textContent = "Reconstructed audio classified successfully!";
                
            } catch (error) {
                messageBox.textContent = "Error: " + error.message;
                console.error('Classification error:', error);
            } finally {
                classifyReconstructedBtn.disabled = false;
            }
        }

        function displayOriginalResults(data) {
            originalGender.textContent = data.gender.toUpperCase();
            originalConfidence.textContent = (data.confidence * 100).toFixed(1) + '%';
            originalPitch.textContent = data.pitch.toFixed(1) + ' Hz';
            originalSampleRate.textContent = data.sampleRate;

            // Reset and apply gender-specific styling
            originalResults.classList.remove('results-male', 'results-female');
            originalAccentBar.classList.remove('gender-accent-bar-male', 'gender-accent-bar-female');

            if (data.gender === 'male') {
                originalResults.classList.add('results-male');
                originalAccentBar.classList.add('gender-accent-bar-male');
                originalGender.className = 'text-2xl font-bold text-blue-400';
            } else {
                originalResults.classList.add('results-female');
                originalAccentBar.classList.add('gender-accent-bar-female');
                originalGender.className = 'text-2xl font-bold text-pink-400';
            }

            originalResults.classList.remove('hidden');
        }

        function displayResampledResults(data) {
            resampledGender.textContent = data.gender.toUpperCase();
            resampledConfidence.textContent = (data.confidence * 100).toFixed(1) + '%';
            resampledPitch.textContent = data.pitch.toFixed(1) + ' Hz';
            resampledSampleRate.textContent = data.sampleRate;

            // Reset and apply gender-specific styling
            resampledResults.classList.remove('results-male', 'results-female');
            resampledAccentBar.classList.remove('gender-accent-bar-male', 'gender-accent-bar-female');

            if (data.gender === 'male') {
                resampledResults.classList.add('results-male');
                resampledAccentBar.classList.add('gender-accent-bar-male');
                resampledGender.className = 'text-2xl font-bold text-blue-400';
            } else {
                resampledResults.classList.add('results-female');
                resampledAccentBar.classList.add('gender-accent-bar-female');
                resampledGender.className = 'text-2xl font-bold text-pink-400';
            }

            resampledResults.classList.remove('hidden');
        }

        function displayReconstructedResults(data) {
            reconstructedGender.textContent = data.gender.toUpperCase();
            reconstructedConfidence.textContent = (data.confidence * 100).toFixed(1) + '%';
            reconstructedPitch.textContent = data.pitch.toFixed(1) + ' Hz';
            reconstructedSampleRate.textContent = data.sampleRate;
            reconstructedOriginalRate.textContent = data.downsampledFrom;

            // Reset and apply gender-specific styling
            reconstructedResults.classList.remove('results-male', 'results-female');
            reconstructedAccentBar.classList.remove('gender-accent-bar-male', 'gender-accent-bar-female');

            if (data.gender === 'male') {
                reconstructedResults.classList.add('results-male');
                reconstructedAccentBar.classList.add('gender-accent-bar-male');
                reconstructedGender.className = 'text-2xl font-bold text-blue-400';
            } else {
                reconstructedResults.classList.add('results-female');
                reconstructedAccentBar.classList.add('gender-accent-bar-female');
                reconstructedGender.className = 'text-2xl font-bold text-pink-400';
            }

            reconstructedResults.classList.remove('hidden');
        }

        function displayAliasingAnalysis() {
            const match = originalClassification.gender === resampledClassification.gender;
            const confChange = ((resampledClassification.confidence - originalClassification.confidence) * 100).toFixed(1);
            const pitchDiff = (resampledClassification.pitch - originalClassification.pitch).toFixed(1);
            const nyquist = (resampledClassification.sampleRate / 2).toFixed(0);

            // Classification match
            classificationMatch.textContent = match ? '✓ MATCH' : '✗ MISMATCH';
            classificationMatch.className = match ? 'font-bold text-lg text-green-400' : 'font-bold text-lg text-red-400';

            // Confidence change
            confidenceChange.textContent = (confChange >= 0 ? '+' : '') + confChange + '%';
            confidenceChange.className = Math.abs(confChange) < 5 ? 'font-bold text-lg text-green-400' : 'font-bold text-lg text-yellow-400';

            // Pitch change
            pitchChange.textContent = (pitchDiff >= 0 ? '+' : '') + pitchDiff + ' Hz';
            pitchChange.className = 'font-bold text-lg text-cyan-400';

            // Nyquist frequency
            nyquistFreq.textContent = nyquist + ' Hz';

            // Aliasing effect description
            let effect = '';
            const sampleRate = resampledClassification.sampleRate;
            
            if (sampleRate < 8000) {
                effect = '⚠️ SEVERE ALIASING: Sample rate is critically low. Significant loss of voice characteristics. High-frequency components are severely distorted.';
            } else if (sampleRate < 12000) {
                effect = '⚠️ MODERATE ALIASING: Sample rate below recommended minimum for voice. Noticeable degradation in audio quality and potential classification errors.';
            } else if (sampleRate < 16000) {
                effect = '⚠️ MILD ALIASING: Sample rate is acceptable but may cause minor artifacts. Some high-frequency voice characteristics may be lost.';
            } else {
                effect = '✓ MINIMAL ALIASING: Sample rate is sufficient for voice classification. Audio quality preserved with minimal artifacts.';
            }

            if (!match) {
                effect += ' ❌ Gender classification changed due to under-sampling effects!';
            }

            aliasingEffect.textContent = effect;
            aliasingAnalysis.classList.remove('hidden');
        }
        
        // --- 4. Playback Functions ---
        
        function stopAudio() {
            if (currentSourceNode) {
                currentSourceNode.stop();
                currentSourceNode = null;
            }
            isPlaying = false;
            // Re-enable appropriate buttons
            if (originalBuffer) {
                playOriginalBtn.disabled = false;
                playSampledBtn.disabled = false;
                playReconstructedBtn.disabled = false;
                classifyOriginalBtn.disabled = false;
                classifyResampledBtn.disabled = false;
                classifyReconstructedBtn.disabled = false;
            }
            pauseBtn.disabled = true;
            messageBox.textContent = "Playback paused/stopped.";
        }
        
        function resetApp() {
            stopAudio();
            
            // Stop any ongoing recording
            if (mediaRecorder && mediaRecorder.state !== 'inactive') {
                mediaRecorder.stop();
            }
            if (recordingStream) {
                recordingStream.getTracks().forEach(track => track.stop());
                recordingStream = null;
            }
            if (recordingTimerInterval) {
                clearInterval(recordingTimerInterval);
                recordingTimerInterval = null;
            }
            
            // Clear data
            originalBuffer = null;
            resampledBuffer = null;
            reconstructedBuffer = null;
            uploadedFile = null;
            isPlaying = false;
            currentSourceNode = null;
            audioChunks = [];
            recordingStartTime = null;
            originalClassification = null;
            resampledClassification = null;
            reconstructedClassification = null;
            
            // Reset recording UI
            recordBtn.disabled = false;
            recordBtn.classList.remove('record-btn-active');
            stopRecordBtn.disabled = true;
            recordingTimer.classList.add('hidden');
            recordingStatus.textContent = '';
            recordingStatus.classList.remove('recording-indicator');
            
            // Reset UI states
            controls.classList.add('opacity-50', 'pointer-events-none');
            playOriginalBtn.disabled = true;
            playSampledBtn.disabled = true;
            playReconstructedBtn.disabled = true;
            pauseBtn.disabled = true;
            resetBtn.disabled = true;
            classifyOriginalBtn.disabled = true;
            classifyResampledBtn.disabled = true;
            classifyReconstructedBtn.disabled = true;
            
            fileNameDisplay.classList.add('hidden');
            filePrompt.classList.remove('hidden');
            filePrompt.innerHTML = '**Drag & Drop** an MP3/WAV audio file here, or **click to upload**. ';
            
            // Reset slider to default/initial values
            const defaultRate = audioContext ? audioContext.sampleRate : 44100;
            frequencySlider.min = 4000;
            frequencySlider.max = defaultRate;
            frequencySlider.value = defaultRate;
            currentFrequencyDisplay.textContent = `${defaultRate} (Original Rate)`;
            
            // Hide all results
            originalResults.classList.add('hidden');
            resampledResults.classList.add('hidden');
            reconstructedResults.classList.add('hidden');
            aliasingAnalysis.classList.add('hidden');
            
            // Remove gender accent classes on full reset
            originalResults.classList.remove('results-male', 'results-female');
            resampledResults.classList.remove('results-male', 'results-female');
            reconstructedResults.classList.remove('results-male', 'results-female');
            originalAccentBar.classList.remove('gender-accent-bar-male', 'gender-accent-bar-female');
            resampledAccentBar.classList.remove('gender-accent-bar-male', 'gender-accent-bar-female');
            reconstructedAccentBar.classList.remove('gender-accent-bar-male', 'gender-accent-bar-female');

            messageBox.textContent = "Application reset. Ready for new file.";
        }

        function playAudio(buffer) {
            if (isPlaying) {
                stopAudio();
            }
            initAudioContext();
            
            currentSourceNode = audioContext.createBufferSource();
            currentSourceNode.buffer = buffer;
            currentSourceNode.connect(audioContext.destination);
            
            currentSourceNode.onended = () => {
                if (isPlaying) {
                    isPlaying = false;
                    currentSourceNode = null;
                    playOriginalBtn.disabled = false;
                    playSampledBtn.disabled = false;
                    playReconstructedBtn.disabled = false;
                    pauseBtn.disabled = true;
                    classifyOriginalBtn.disabled = false;
                    classifyResampledBtn.disabled = false;
                    classifyReconstructedBtn.disabled = false;
                    messageBox.textContent = "Playback finished.";
                }
            };

            currentSourceNode.start(0);
            isPlaying = true;
            playOriginalBtn.disabled = true;
            playSampledBtn.disabled = true;
            playReconstructedBtn.disabled = true;
            classifyOriginalBtn.disabled = true; // Disable classification during playback
            classifyResampledBtn.disabled = true;
            classifyReconstructedBtn.disabled = true;
            pauseBtn.disabled = false;
            messageBox.textContent = "Playing audio...";
        }

        // --- 5. Voice Recording Functions ---

        async function startRecording() {
            try {
                // Request microphone access
                recordingStream = await navigator.mediaDevices.getUserMedia({ 
                    audio: {
                        channelCount: 1,
                        sampleRate: 16000,
                        echoCancellation: true,
                        noiseSuppression: true
                    } 
                });

                audioChunks = [];
                
                // Create MediaRecorder with audio/webm format
                const options = { mimeType: 'audio/webm' };
                mediaRecorder = new MediaRecorder(recordingStream, options);

                mediaRecorder.ondataavailable = (event) => {
                    if (event.data.size > 0) {
                        audioChunks.push(event.data);
                    }
                };

                mediaRecorder.onstop = async () => {
                    // Create blob from chunks
                    const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
                    
                    // Convert to WAV format
                    await convertToWavAndLoad(audioBlob);
                    
                    // Stop all tracks
                    if (recordingStream) {
                        recordingStream.getTracks().forEach(track => track.stop());
                        recordingStream = null;
                    }
                };

                // Start recording
                mediaRecorder.start();
                recordingStartTime = Date.now();
                
                // Update UI
                recordBtn.disabled = true;
                recordBtn.classList.add('record-btn-active');
                stopRecordBtn.disabled = false;
                recordingTimer.classList.remove('hidden');
                recordingStatus.textContent = '🔴 Recording...';
                recordingStatus.classList.add('recording-indicator');
                
                // Start timer
                recordingTimerInterval = setInterval(updateRecordingTimer, 100);
                
            } catch (error) {
                console.error('Error accessing microphone:', error);
                recordingStatus.textContent = 'Error: Could not access microphone. Please grant permission.';
                recordingStatus.classList.remove('recording-indicator');
            }
        }

        function stopRecording() {
            if (mediaRecorder && mediaRecorder.state !== 'inactive') {
                mediaRecorder.stop();
                
                // Update UI
                recordBtn.disabled = false;
                recordBtn.classList.remove('record-btn-active');
                stopRecordBtn.disabled = true;
                recordingTimer.classList.add('hidden');
                recordingStatus.textContent = 'Processing recording...';
                recordingStatus.classList.remove('recording-indicator');
                
                // Stop timer
                if (recordingTimerInterval) {
                    clearInterval(recordingTimerInterval);
                    recordingTimerInterval = null;
                }
            }
        }

        function updateRecordingTimer() {
            if (recordingStartTime) {
                const elapsed = Math.floor((Date.now() - recordingStartTime) / 1000);
                const minutes = Math.floor(elapsed / 60);
                const seconds = elapsed % 60;
                recordingTimer.textContent = `${String(minutes).padStart(2, '0')}:${String(seconds).padStart(2, '0')}`;
            }
        }

        async function convertToWavAndLoad(webmBlob) {
            try {
                initAudioContext();
                
                // Read the webm blob as array buffer
                const arrayBuffer = await webmBlob.arrayBuffer();
                
                // Decode the audio data
                const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
                
                // Convert AudioBuffer to WAV format
                const wavBlob = audioBufferToWav(audioBuffer);
                
                // Create a File object from the WAV blob
                const wavFile = new File([wavBlob], 'recorded_voice.wav', { type: 'audio/wav' });
                
                // Store for classification
                uploadedFile = wavFile;
                
                // Load into the app
                decodeAudio(wavFile);
                
                recordingStatus.textContent = '✓ Recording saved successfully!';
                setTimeout(() => {
                    recordingStatus.textContent = '';
                }, 3000);
                
            } catch (error) {
                console.error('Error converting recording:', error);
                recordingStatus.textContent = 'Error processing recording.';
            }
        }

        function audioBufferToWav(buffer) {
            const length = buffer.length * buffer.numberOfChannels * 2 + 44;
            const arrayBuffer = new ArrayBuffer(length);
            const view = new DataView(arrayBuffer);
            const channels = [];
            let offset = 0;
            let pos = 0;

            // Write WAV header
            const setUint16 = (data) => {
                view.setUint16(pos, data, true);
                pos += 2;
            };
            const setUint32 = (data) => {
                view.setUint32(pos, data, true);
                pos += 4;
            };

            // "RIFF" chunk descriptor
            setUint32(0x46464952); // "RIFF"
            setUint32(length - 8); // file length - 8
            setUint32(0x45564157); // "WAVE"

            // "fmt " sub-chunk
            setUint32(0x20746d66); // "fmt "
            setUint32(16); // SubChunk1Size = 16
            setUint16(1); // AudioFormat = 1 (PCM)
            setUint16(buffer.numberOfChannels);
            setUint32(buffer.sampleRate);
            setUint32(buffer.sampleRate * buffer.numberOfChannels * 2); // ByteRate
            setUint16(buffer.numberOfChannels * 2); // BlockAlign
            setUint16(16); // BitsPerSample

            // "data" sub-chunk
            setUint32(0x61746164); // "data"
            setUint32(length - pos - 4); // SubChunk2Size

            // Write interleaved data
            for (let i = 0; i < buffer.numberOfChannels; i++) {
                channels.push(buffer.getChannelData(i));
            }

            while (pos < length) {
                for (let i = 0; i < buffer.numberOfChannels; i++) {
                    let sample = Math.max(-1, Math.min(1, channels[i][offset]));
                    sample = sample < 0 ? sample * 0x8000 : sample * 0x7FFF;
                    view.setInt16(pos, sample, true);
                    pos += 2;
                }
                offset++;
            }

            return new Blob([arrayBuffer], { type: 'audio/wav' });
        }

        // --- 6. Event Listeners ---

        // Recording button handlers
        recordBtn.addEventListener('click', startRecording);
        stopRecordBtn.addEventListener('click', stopRecording);

        // File Input Change
        fileInput.addEventListener('change', (e) => {
            if (e.target.files.length > 0) {
                decodeAudio(e.target.files[0]);
            }
        });

        // Drag and Drop handlers
        ['dragenter', 'dragover', 'dragleave', 'drop'].forEach(eventName => {
            dragDropArea.addEventListener(eventName, preventDefaults, false);
            document.body.addEventListener(eventName, preventDefaults, false);
        });

        function preventDefaults(e) {
            e.preventDefault();
            e.stopPropagation();
        }

        ['dragenter', 'dragover'].forEach(eventName => {
            dragDropArea.addEventListener(eventName, () => dragDropArea.classList.add('drag-over'), false);
        });

        ['dragleave', 'drop'].forEach(eventName => {
            dragDropArea.addEventListener(eventName, () => dragDropArea.classList.remove('drag-over'), false);
        });

        dragDropArea.addEventListener('drop', (e) => {
            const dt = e.dataTransfer;
            const files = dt.files;
            if (files.length > 0 && files[0].type.startsWith('audio/')) {
                decodeAudio(files[0]);
            } else {
                messageBox.textContent = "Please drop a valid audio file (mp3, wav).";
            }
        }, false);

        // Slider Input Change
        frequencySlider.addEventListener('input', (e) => {
            const targetRate = parseInt(e.target.value, 10);
            currentFrequencyDisplay.textContent = targetRate;
            
            if (targetRate === originalRate) {
                currentFrequencyDisplay.textContent += ' (Original Rate)';
            } else if (targetRate < 8000) {
                currentFrequencyDisplay.textContent += ' (Severe Degradation)';
            }
            
            if (originalBuffer) {
                resampleAudio(targetRate);
            }
        });

        // Playback button handlers
        playOriginalBtn.addEventListener('click', () => {
            if (originalBuffer) {
                playAudio(originalBuffer);
            }
        });

        playSampledBtn.addEventListener('click', () => {
            if (resampledBuffer) {
                playAudio(resampledBuffer);
            }
        });

        playReconstructedBtn.addEventListener('click', () => {
            if (reconstructedBuffer) {
                playAudio(reconstructedBuffer);
            } else {
                messageBox.textContent = "No reconstructed audio available! Enable anti-aliasing filter and adjust sample rate.";
            }
        });
        
        // Control buttons
        pauseBtn.addEventListener('click', stopAudio);
        resetBtn.addEventListener('click', resetApp);
        classifyOriginalBtn.addEventListener('click', classifyOriginal);
        classifyResampledBtn.addEventListener('click', classifyResampled);
        classifyReconstructedBtn.addEventListener('click', classifyReconstructed);

        // Anti-aliasing toggle handler
        antiAliasingToggle.addEventListener('change', () => {
            if (originalBuffer) {
                const targetRate = parseInt(frequencySlider.value);
                resampleAudio(targetRate);
                messageBox.textContent = antiAliasingToggle.checked ? 
                    "Anti-aliasing filter enabled. Signal will be low-pass filtered before downsampling." : 
                    "Anti-aliasing filter disabled. Direct downsampling (aliasing may occur).";
            }
        });

        // Initial setup on load
        document.addEventListener('DOMContentLoaded', initAudioContext);
    </script>
</body>
</html>